---
layout:     post
title:    导数 微分 偏导数 方向导数 梯度 梯度下降法总结
subtitle:   基础知识复习
date:       2018-06-12
author:     粘世强
header-img: img/lighthouse.jpg
catalog: true
tags:
        - machine learning
---

在机器学习中，首先学习到的算法便是梯度下降法，这其中涉及到的相关导数和微分及梯度等基础知识，在这里整理一下，主要内容来自某一博客还有吴恩达的机器学习视频。这篇博客写的很出色，讲解很清晰，因此保存下来，但是博客地址找不见了，如果博主看到，麻烦通知一下，我加上转载地址。谢谢。

![](https://github.com/nianshiqiang/nianshiqiang.github.io/blob/master/contentimg/%E5%AF%BC%E6%95%B0%E5%BE%AE%E5%88%86%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/1.png?raw=true)

![](https://github.com/nianshiqiang/nianshiqiang.github.io/blob/master/contentimg/%E5%AF%BC%E6%95%B0%E5%BE%AE%E5%88%86%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/2.jpg?raw=true)

![](https://github.com/nianshiqiang/nianshiqiang.github.io/blob/master/contentimg/%E5%AF%BC%E6%95%B0%E5%BE%AE%E5%88%86%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/3.png?raw=true)

![](https://github.com/nianshiqiang/nianshiqiang.github.io/blob/master/contentimg/%E5%AF%BC%E6%95%B0%E5%BE%AE%E5%88%86%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/4.jpg?raw=true)

![](https://github.com/nianshiqiang/nianshiqiang.github.io/blob/master/contentimg/%E5%AF%BC%E6%95%B0%E5%BE%AE%E5%88%86%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/5.png?raw=true)

从上边的图中可以看出，在梯度下降法中，选择不同的起始值，会得到不同的局部最小值（凸函数除外）。因此，利用梯度下降法时，要注意得到的是局部最小值。

![](https://github.com/nianshiqiang/nianshiqiang.github.io/blob/master/contentimg/%E5%AF%BC%E6%95%B0%E5%BE%AE%E5%88%86%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/6.png?raw=true)

从上边的图中可以看出，如果α值过小，则需要很多步才能收敛到最小值，如果α值过大，则很可能直接越过最小值，导致无法收敛。因此，在使用梯度下降法过程中，一定要注意调整学习率α。

![](https://github.com/nianshiqiang/nianshiqiang.github.io/blob/master/contentimg/%E5%AF%BC%E6%95%B0%E5%BE%AE%E5%88%86%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/7.png?raw=true)

随着梯度下降法的进行，越来越接近最低点，导数项越来越小，因此θ每次更新的幅度也会越来越小，直到最终移动幅度非常小，此时已经到达最小值。在梯度下降法中，当我们接近局部最低点时，梯度下降法会自动采取更小的幅度，很显然在局部最低时导数等于零，所以当我们接近局部最低时，导数值会自动变得越来越小，所以梯度下降将自动采取较小的幅度，这就是梯度下降的做法。所以实际上没有必要再另外减小α。 

梯度下降算法可以用来最小化任何的代价函数，不只是线性回归中的代价函数。