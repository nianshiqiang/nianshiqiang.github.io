---
layout:     post
title:     机器学习：随机森林模型详解
subtitle:   Random Forest
date:       2018-12-06
author:     粘世强
header-img: img/staircase.jpg
catalog: true
tags:
    - machine learning
---
## 随机森林模型

随机森林，顾名思义，是用随机的方式建立一个森林，森林里面有很多的决策树，随机森林的每一棵决策树之间是没有关联的。在得到森林之后，当有一个新的输 入样本进入的时候，就让森林中的每一棵决策树分别进行一下判断，看看这个样本应该属于哪一类（对于分类算法），然后看看哪一类被选择最多，就预测这个样本为那一类。随机森林由决策树组成，决策树实际上是将空间用超平面进行划分的一种方法，每次分割的时候，都将当前的空间一分为二。

## 采样与完全分裂

在建立每一棵决策树的过程中，有两点需要注意 - 采样与完全分裂。首先是两个随机采样的过程，random forest对输入的数据要进行行、列的采样。对于行采样，采用有放回的方式（bootstrap)，也就是在采样得到的样本集合中，可能有重复的样本。假设输入样本为N个，那么采样的样本也为N个。这样使得在训练的时候，每一棵树的输入样本都不是全部的样本，使得相对不容易出现过拟合。然后进行列采样，从M 个feature中，选择m个(m << M，推荐为M的平方根)。之后就是对采样之后的数据使用完全分裂的方式建立出决策树，这样决策树的某一个叶子节点要么是无法继续分裂的，要么里面的所有样本的都是指向的同一个分类，或者是达到了预设的条件（如min_sample_leaf或者max_depth等）。

按这种算法得到的随机森林中的每一棵都是很弱的，但是大家组合起来就很厉害了。我觉得可以这样比喻随机森林算法：每一棵决策树就是一个精通于某一个窄领域的专家（因为我们从M个feature中选择m让每一棵决策树进行学习），这样在随机森林中就有了很多个精通不同领域的专家，对一个新的问题（新的输入数据），可以用不同的角度去看待它，最终由各个专家，投票得到结果。

综上所述：随机森林中的每一棵决策树，样本数量和总样本数量一致（通过bootstrap方法，有放回采样），样本的特征数为m个（从M各特征中随机抽取m个特征）。

## 随机森林的随机性主要体现在两个方面：

**数据集的随机选取：**从原始的数据集中采取有放回的抽样，构造子数据集，子数据集的数据量是和原始数据集相同的。不同子数据集的元素可以重复，同一个子数据集中的元素也可以重复。

为什么要选择有放回的采样方式？

我的理解：如果不采用有放回的方式，第一种可能是每棵树的样本都是样本全集，这种情况下得到的每棵树都是一样的。第二种可能是从数据集中进行随机采样，得到不同的训练子集，这样每棵树的训练数据集都完全不同，而且每棵树仅仅利用了一小部分数据，甚至都不足以进行有效学习。

因此采用这种bootstrap方法，  既保证了每棵树之间的数据具有一定的差异，同时每棵树的训练数据量又和样本总量相同，不会因为数据量过少而导致每棵树的学习能力变弱。

**待选特征的随机选取：**与数据集的随机选取类似，随机森林中的子树的每一个分裂过程并未用到所有的待选特征，而是从所有的待选特征中随机选取一定的特征，之后再在随机选取的特征中选取最优的特征。

## 随机森林的优缺点：

随机森林的优点：

- 实现简单，训练速度快，泛化能力强，可以并行实现，因为训练时树与树之间是相互独立的；
- 相比单一决策树，能学习到特征之间的相互影响，且不容易过拟合；
- 能处理高维数据（即特征很多），并且不用做特征选择，因为特征子集是随机选取的；
- 对于不平衡的数据集，可以平衡误差；
- 相比SVM，不是很怕特征缺失，因为待选特征也是随机选取；
- 训练完成后可以给出哪些特征比较重要。

随机森林的缺点：

- 在噪声过大的分类和回归问题还是容易过拟合；
- 相比于单一决策树，虽然提高了准确度，但是它的随机性让我们难以对模型进行解释。

## 随机森林与一般的bagging相比：

bagging方法的的随机性仅仅来自样本扰动，随机林模型中引入了属性扰动，这样使得最终模型的泛化性能可以通过个体学习器之间的差异度的增加而进一步提升。

和bagging相比，随机森林的起始性能往往比较差，然而随着个体学习器数目的增加，随机森林会收敛到更小的误差。

随机森林的训练效率优于bagging，因为bagging中的每棵树是对所有特征进行考察，而随机森林仅仅考虑一个特征子集。

## sklearn中随机森林模型中的参数解读：

**A. n_estimators**

在利用最大投票数或平均值来预测之前，你想要建立子树的数量。 较多的子树可以让模型有更好的性能，但同时让你的代码变慢。 你应该选择尽可能高的值，只要你的处理器能够承受的住，因为这使你的预测更好更稳定。 

**B. max_features**

随机森林允许单个决策树使用特征的最大数量。 Python为最大特征数提供了多个可选项。 下面是其中的几个：

- 如果是整数a，则考虑a个特征
- 如果是小数，eg 0.2，允许每个随机森林的子树可以利用变量（特征）数的20％

- If “auto”, then max_features=sqrt(n_features).（默认选项）
- If “sqrt”, then max_features=sqrt(n_features) (和“auto”功能一致).
- If “log2”, then max_features=log2(n_features).
- If None, then max_features=n_features（选择所有特征）

max_features如何影响性能和速度？

增加max_features一般能提高模型的性能，因为在每个节点上，我们有更多的选择可以考虑。 然而，这未必完全是对的，因为它降低了单个树的多样性，而这正是随机森林独特的优点。 但是，可以肯定，你通过增加max_features会降低算法的速度。 因此，你需要适当的平衡和选择最佳max_features。 

**C. min_sample_leaf**

每一个叶子节点上的最小样本数。叶是决策树的末端节点。 较小的叶子使模型更容易捕捉训练数据中的噪声。 类似于决策树中的剪枝操作。默认是1，可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。

**D:criterion**

划分标准：gini或者entropy

**E:max_depth**

 默认可以不输入，如果不输入的话，决策树在建立子树的时候不会限制子树的深度。直到每个叶子节点的样本为一类或者达到min_sample_leaf。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间。

**F:min_samples_split**

内部节点再划分所需最小样本数min_samples_split: 这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。 默认是2.如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。

**G:max_leaf_nodes**

 最大叶子节点数max_leaf_nodes: 通过限制最大叶子节点数，可以防止过拟合，默认是”None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的值可以通过交叉验证得到。

## 参考资料：

1. <https://blog.csdn.net/HHTNAN/article/details/80237226>
2. <https://blog.csdn.net/login_sonata/article/details/73929426>
3. 周志华老师的《机器学习》